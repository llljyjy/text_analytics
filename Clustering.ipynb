{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import string\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess2\n",
    "\n",
    "reviews_corpus = preprocess2.load_corpus('corpus_lemma')\n",
    "reviews_docs = preprocess2.corpus2docs(reviews_corpus)\n",
    "reviews_docs_joined = [\" \".join(x) for x in reviews_docs]\n",
    "\n",
    "\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", max_features=1000, stop_words = 'english', ngram_range=(1,1), analyzer='word')  # You can adjust the number of features as needed\n",
    "\n",
    "# Fit and transform your preprocessed text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(reviews_docs_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Import the necessary libraries (if not already imported)\n",
    "n_components = 2  # Number of components for TruncatedSVD\n",
    "n_clusters_range = range(1, 10)  # Adjust the range as needed\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=0)\n",
    "Y_svd = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "kmeans = [KMeans(n_clusters=i, max_iter=600) for i in n_clusters_range]\n",
    "\n",
    "score = [kmeans[i].fit(Y_svd).inertia_ for i in range(len(kmeans))]  # Added a closing parenthesis here\n",
    "\n",
    "plt.plot(n_clusters_range, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Choose the number of clusters (k)\n",
    "k = 3  # You can adjust the number of clusters based on your problem\n",
    "\n",
    "# Step 3: Apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Get the cluster assignments for each document\n",
    "cluster_assignments = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. Print the cluster assignments for each document\n",
    "for i in range(k):\n",
    "    cluster_i_indices = np.where(cluster_assignments == i)[0]\n",
    "    print(f\"Cluster {i} documents:\")\n",
    "    for doc_index in cluster_i_indices:\n",
    "        print(reviews_docs_joined[doc_index])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2. Analyze cluster centroids\n",
    "cluster_centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3. Get the top terms for each cluster\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "order_centroids = cluster_centroids.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"Cluster {i} top terms:\")\n",
    "    top_terms = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "    print(top_terms)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4. Evaluate the clustering quality (if you have ground truth labels)\n",
    "# You can use metrics like silhouette score, completeness, or homogeneity.\n",
    "\n",
    "#silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(tfidf_matrix, cluster_assignments)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(reviews_docs, min_count=5, threshold=100) # higher threshold fewer phrases. \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_bigrams = make_bigrams(reviews_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TAA2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
