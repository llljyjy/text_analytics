{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess2\n",
    "reviews_corpus = preprocess2.load_corpus('sephora_corpus')\n",
    "reviews_docs = preprocess2.corpus2docs(reviews_corpus)\n",
    "reviews_docs_joined = [\" \".join(x) for x in reviews_docs]  #joined to fit vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(verbose=True, n_gram_range=(1,1))\n",
    "topics, _ = topic_model.fit_transform(reviews_docs_joined); len(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from gensim import corpora\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import CoherenceModel\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "import gensim.corpora as corpora\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the range of parameters\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Dimensionality Reduction Parameters\n",
    "n_neighbors_range = [5, 10, 15]\n",
    "n_components_range = [3, 5, 7]\n",
    "\n",
    "model_results = {'Topics': [],\n",
    "                 'N_Neighbors': [],\n",
    "                 'N_Components': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Define the size of the subset (10%)\n",
    "subset_size = int(len(reviews_docs_joined) * 0.1)\n",
    "subset_docs = np.random.choice(reviews_docs_joined, subset_size, replace=False)\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(topics_range) * len(n_neighbors_range) * len(n_components_range)))\n",
    "\n",
    "for k in topics_range:\n",
    "    for n_neighbors in n_neighbors_range:\n",
    "        for n_components in n_components_range:\n",
    "            # Initialize the BERTopic model with specified hyperparameters\n",
    "            embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "            umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, metric='manhattan', low_memory=True)\n",
    "            hdbscan_model = HDBSCAN()\n",
    "            vectorizer_model = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "            ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "            representation_model = KeyBERTInspired()\n",
    "            \n",
    "            # Create the BERTopic model\n",
    "            topic_model = BERTopic(embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer_model, ctfidf_model=ctfidf_model, representation_model=representation_model)\n",
    "            \n",
    "            # Fit the model on your subset of documents\n",
    "            topics, _ = topic_model.fit_transform(subset_docs)\n",
    "            \n",
    "            # Preprocess documents\n",
    "            documents = pd.DataFrame(\n",
    "                {\"Document\": subset_docs,\n",
    "                 \"ID\": range(len(subset_docs)),\n",
    "                 \"Topic\": topics}\n",
    "            )\n",
    "            documents_per_topic = documents.groupby(\n",
    "                ['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "            cleaned_docs = topic_model._preprocess_text(\n",
    "                documents_per_topic.Document.values)\n",
    "            \n",
    "            # Extract vectorizer and analyzer from the fitted model\n",
    "            vectorizer_model = topic_model.vectorizer_model\n",
    "            analyzer = vectorizer_model.build_analyzer()\n",
    "            \n",
    "            # Extract features for topic coherence evaluation\n",
    "            tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "            dictionary = corpora.Dictionary(tokens)\n",
    "            corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "            topic_words = [[words for words, _ in topic_model.get_topic(topic)] for topic in range(len(set(topics)) - 1)]\n",
    "            \n",
    "            # Calculate coherence\n",
    "            coherence_model = CoherenceModel(topics=topic_words, texts=tokens, corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_score = coherence_model.get_coherence()\n",
    "            \n",
    "            # Save the model results\n",
    "            model_results['Topics'].append(k)\n",
    "            model_results['N_Neighbors'].append(n_neighbors)\n",
    "            model_results['N_Components'].append(n_components)\n",
    "            model_results['Coherence'].append(coherence_score)\n",
    "\n",
    "            pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_results).to_csv('./bertopic_tuning_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import string\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess2\n",
    "\n",
    "reviews_corpus = preprocess2.load_corpus('corpus_lemma')\n",
    "reviews_docs = preprocess2.corpus2docs(reviews_corpus)\n",
    "reviews_docs_joined = [\" \".join(x) for x in reviews_docs]\n",
    "\n",
    "\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", max_features=1000, stop_words = 'english', ngram_range=(1,1), analyzer='word')  # You can adjust the number of features as needed\n",
    "\n",
    "# Fit and transform your preprocessed text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(reviews_docs_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Import the necessary libraries (if not already imported)\n",
    "n_components = 2  # Number of components for TruncatedSVD\n",
    "n_clusters_range = range(1, 10)  # Adjust the range as needed\n",
    "\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=0)\n",
    "Y_svd = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "kmeans = [KMeans(n_clusters=i, max_iter=600) for i in n_clusters_range]\n",
    "\n",
    "score = [kmeans[i].fit(Y_svd).inertia_ for i in range(len(kmeans))]  # Added a closing parenthesis here\n",
    "\n",
    "plt.plot(n_clusters_range, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Choose the number of clusters (k)\n",
    "k = 3  # You can adjust the number of clusters based on your problem\n",
    "\n",
    "# Step 3: Apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "kmeans.fit(tfidf_matrix)\n",
    "\n",
    "# Get the cluster assignments for each document\n",
    "cluster_assignments = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. Print the cluster assignments for each document\n",
    "for i in range(k):\n",
    "    cluster_i_indices = np.where(cluster_assignments == i)[0]\n",
    "    print(f\"Cluster {i} documents:\")\n",
    "    for doc_index in cluster_i_indices:\n",
    "        print(reviews_docs_joined[doc_index])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2. Analyze cluster centroids\n",
    "cluster_centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3. Get the top terms for each cluster\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "order_centroids = cluster_centroids.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(k):\n",
    "    print(f\"Cluster {i} top terms:\")\n",
    "    top_terms = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "    print(top_terms)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4. Evaluate the clustering quality (if you have ground truth labels)\n",
    "# You can use metrics like silhouette score, completeness, or homogeneity.\n",
    "\n",
    "#silhouette score\n",
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_avg = silhouette_score(tfidf_matrix, cluster_assignments)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
