{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "from ast import literal_eval\n",
    "from matplotlib import colormaps\n",
    "import squarify\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import gensim\n",
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess2\n",
    "reviews_corpus = preprocess2.load_corpus('sephora_corpus')\n",
    "reviews_docs = preprocess2.corpus2docs(reviews_corpus)\n",
    "reviews_docs_joined = [\" \".join(x) for x in reviews_docs]  #joined to fit vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the data to get only 30% of it\n",
    "# _, sampled_reviews_docs_joined, _, sampled_reviews_docs = train_test_split(reviews_docs_joined, reviews_docs, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (based on prev section: k = 6, alpha = 0.9099999999999999 and beta = 0.9099999999999999) + KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import preprocess2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a Dictionary\n",
    "id2word = corpora.Dictionary(reviews_docs)\n",
    "\n",
    "# Create a Corpus\n",
    "texts = reviews_docs\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(doc) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Document-Word matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "data = vectorizer.fit_transform(reviews_docs_joined)\n",
    "\n",
    "# Extract features\n",
    "features = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aaaaah' ... 'zumba' 'zunc' 'zyleer']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_topic(corpus, model):\n",
    "    doc_topic = list()\n",
    "    for doc in corpus:\n",
    "        doc_topic.append(model.__getitem__(doc, eps=0))\n",
    "        return doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=6, \n",
    "                                       random_state=100,\n",
    "                                       alpha = 0.9099999999999999,\n",
    "                                       eta = 0.9099999999999999,\n",
    "                                       per_word_topics=True, \n",
    "                                       chunksize=100,\n",
    "                                       passes=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data = get_doc_topic(corpus, optimal_lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def gen_init_point(docTopic, docWord, a):\n",
    "\n",
    "    thershold = int((docTopic.shape[0] // docTopic.shape[1]) + a * docTopic.shape[0])\n",
    "\n",
    "    # print(thershold)\n",
    "\n",
    "    topic_mean = docTopic.mean(axis=0)\n",
    "    # print(topic_mean)\n",
    "    # print(topic_mean)\n",
    "\n",
    "    support_doc_n = []\n",
    "    support_doc_index = []\n",
    "\n",
    "    for x in range(docTopic.shape[1]):\n",
    "        topic = docTopic[:, x]\n",
    "        res_list = topic > topic_mean[x]\n",
    "        res_index = np.where(res_list == True)\n",
    "        support_doc_n.append(len(res_index[0]))\n",
    "        support_doc_index.append(res_index[0])\n",
    "\n",
    "    # print(support_doc_index)\n",
    "    # print(support_doc_n)\n",
    "    support_doc_n = np.array(support_doc_n)\n",
    "    # print(support_doc_n)\n",
    "    typical_topic = np.where(support_doc_n > thershold)[0]\n",
    "    # print(typical_topic)\n",
    "    # print(typical_topic)\n",
    "\n",
    "    k_clustering_init = []\n",
    "    for i in typical_topic:\n",
    "        # print(i)\n",
    "        # print(support_doc_index[i])\n",
    "        # print(dataset[support_doc_index[i]])\n",
    "        k_clustering_init.append(np.asarray(docWord[support_doc_index[i]].mean(axis=0)).reshape(-1))\n",
    "    return np.array(k_clustering_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words, path):\n",
    "    print('for reduce dimension')\n",
    "    out = open(path, 'w')\n",
    "    # model.components_ = lsa.inverse_transform(model.components_)\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        out.write(message+'\\n')\n",
    "    out.close()\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_cluster(model, feature, n, path):\n",
    "    print('for cluster')\n",
    "    out = open(path, 'w')\n",
    "    for topic_idx, topic in enumerate(model.cluster_centers_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature[i]\n",
    "                             for i in topic.argsort()[:-n - 1:-1]])\n",
    "        print(message)\n",
    "        out.write(message+'\\n')\n",
    "    out.close()\n",
    "    print()\n",
    "\n",
    "\n",
    "def output_result(corpus, result, path):\n",
    "    df = pd.DataFrame({'text': corpus, 'label': result})\n",
    "    df[['text', 'label']].to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def pipeline(model, n_topic, data, a, corpus, feature):\n",
    "    lda = model(n_components=n_topic)\n",
    "    data_lda = lda.fit_transform(data)\n",
    "    print(f\"Number of topics: {n_topic}\")\n",
    "    n_init_clusters = gen_init_point(data_lda, data, a)\n",
    "     # Check if n_init_clusters is empty, if yes, skip this iteration\n",
    "    if n_init_clusters.size == 0:\n",
    "        print(f\"Skipping for n_topic={n_topic} due to empty initial clusters\")\n",
    "        return None\n",
    "    # print(n_init_clusters)\n",
    "    km = KMeans(n_clusters=len(n_init_clusters), init=n_init_clusters)\n",
    "    km.fit_transform(data)\n",
    "    print('score')\n",
    "    # Check for the number of unique clusters\n",
    "    unique_labels = np.unique(km.labels_)\n",
    "\n",
    "    if len(unique_labels) > 1:\n",
    "        print('score')\n",
    "        print(silhouette_score(data, km.labels_))\n",
    "      \n",
    "    else:\n",
    "        print(\"Only one cluster found. Cannot compute silhouette score.\")\n",
    "        return None \n",
    "    print_top_words(lda, feature, 10, '_topic_word')\n",
    "    print_cluster(km, feature, 10, '_cluster_meaning_{}')\n",
    "    output_result(corpus, km.labels_, '_cluster_result_{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score\n",
      "0.006472254071764471\n",
      "for reduce dimension\n",
      "Topic #0: bear gummy josie maran revived repeat penetrates trash relieved refrigerator\n",
      "Topic #1: product received skin sample influenster review free honest feel exchange\n",
      "Topic #2: recovery midnight cloud florida omegarich flavour weighed trendmood dense creaminess\n",
      "Topic #3: que peau sha gua jai produit pour pa bien est\n",
      "Topic #4: skin product acne using use week ive love face spot\n",
      "Topic #5: skin love product feel cream great moisturizer serum really using\n",
      "Topic #6: tan tanner self tanning orange color gradual drop lotion tropez\n",
      "Topic #7: blade razor loveeeee polish lite penetrate witch blender floor sharp\n",
      "Topic #8: zenovia ceramide pad minimizes repairing cotton stars oat smelly faster\n",
      "Topic #9: cleanser skin makeup clean love face feel gentle product soft\n",
      "Topic #10: giftedbydermalogica foamy sephoracanada dermalogicacanada kate pool somerville chlorine dog shadow\n",
      "Topic #11: bye fluffy bader nonsticky wipes augustinus supply fourth funky functional\n",
      "Topic #12: lip balm love mask gloss smell chapped amazing best taste\n",
      "Topic #13: eye cream circle dark line product area fine difference around\n",
      "Topic #14: mask mud inkey vaseline list clay aquaphor restock secret waso\n",
      "Topic #15: skin sunscreen product like love moisturizer face great feel doesnt\n",
      "Topic #16: algae renewal triple algenist exfoliator peroxide benzoyl balm brush eyeliner\n",
      "Topic #17: clinical correcting smart repair clinique wrinkle project loveee innbeauty revitalizing\n",
      "Topic #18: price product smell worth money like nothing better dont waste\n",
      "Topic #19: roth peter thomas roller teeth whitening congestion underrated depuffing starter\n",
      "\n",
      "for cluster\n",
      "Topic #0: skin love feel product dry moisturizer great using soft make\n",
      "Topic #1: received product sample skin influenster free review honest exchange feel\n",
      "Topic #2: eye cream product circle dark area love using really around\n",
      "Topic #3: product skin face like love use smell using really good\n",
      "Topic #4: lip balm love mask smell product feel dry soft like\n",
      "Topic #5: sunscreen white cast skin doesnt like love makeup great leave\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topic = 20\n",
    "pipeline(LatentDirichletAllocation,n_topic,data,0.1,sampled_reviews_docs_joined,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 4\n",
      "score\n",
      "score\n",
      "0.0038222136755258853\n",
      "for reduce dimension\n",
      "Topic #0: skin product acne using use ive week used like really\n",
      "Topic #1: skin sunscreen product love moisturizer like great feel oily doesnt\n",
      "Topic #2: skin eye product cream love feel serum using great use\n",
      "Topic #3: skin lip cleanser love face mask feel product makeup like\n",
      "\n",
      "for cluster\n",
      "Topic #0: product skin love like face use smell lip great really\n",
      "Topic #1: skin product feel cream love eye great using really dry\n",
      "\n",
      "Number of topics: 5\n",
      "score\n",
      "score\n",
      "0.003887244604529997\n",
      "for reduce dimension\n",
      "Topic #0: skin lip product like love face use dry smell one\n",
      "Topic #1: sunscreen white cast spf sun love leave skin makeup doesnt\n",
      "Topic #2: skin product love use glow tan look amazing face great\n",
      "Topic #3: skin product love feel great cream really like face moisturizer\n",
      "Topic #4: eye dark product skin using acne week spot difference line\n",
      "\n",
      "for cluster\n",
      "Topic #0: product skin love like face use smell lip really great\n",
      "Topic #1: skin product feel love cream eye great using really dry\n",
      "\n",
      "Number of topics: 6\n",
      "score\n",
      "score\n",
      "0.0044744393728807855\n",
      "for reduce dimension\n",
      "Topic #0: peau que produit pour jai piel est sha bien pa\n",
      "Topic #1: eye product skin cream like sunscreen received feel really using\n",
      "Topic #2: makeup like product skin smell face balm cleansing love remove\n",
      "Topic #3: lip skin love cream feel product moisturizer great like smell\n",
      "Topic #4: skin love product feel face great using soft dry amazing\n",
      "Topic #5: skin product acne use using love face ive like work\n",
      "\n",
      "for cluster\n",
      "Topic #0: eye cream product circle dark area love around using really\n",
      "Topic #1: skin product feel love dry great moisturizer using face really\n",
      "Topic #2: product skin love like use face smell lip really good\n",
      "\n",
      "Number of topics: 7\n",
      "score\n",
      "score\n",
      "0.004472822337491592\n",
      "for reduce dimension\n",
      "Topic #0: skin love makeup feel sunscreen product face great cleanser feeling\n",
      "Topic #1: skin product eye using cream love feel use serum week\n",
      "Topic #2: lip balm mask love smell gloss taste like good best\n",
      "Topic #3: special nothing peau produit price overpriced worth jai meh pour\n",
      "Topic #4: product like skin face smell bad get dont didnt money\n",
      "Topic #5: skin cleanser acne gentle product sensitive dermalogica use love great\n",
      "Topic #6: skin product like moisturizer love smell feel cream great scent\n",
      "\n",
      "for cluster\n",
      "Topic #0: skin product feel love dry great moisturizer using face really\n",
      "Topic #1: eye cream product circle dark area love around using really\n",
      "Topic #2: product skin love like use face smell lip really good\n",
      "\n",
      "Number of topics: 8\n",
      "score\n",
      "score\n",
      "0.003528333774314255\n",
      "for reduce dimension\n",
      "Topic #0: skin product acne using use ive love dry week sensitive\n",
      "Topic #1: skin love feel product moisturizer great amazing face dry make\n",
      "Topic #2: bomb aqua nothing dark underarms corrective armpit ehh mega spot\n",
      "Topic #3: cleanser makeup clean cleansing remove face skin gentle balm wash\n",
      "Topic #4: lip smell sunscreen like white cast love product balm good\n",
      "Topic #5: product packaging money bottle worth like price formula one get\n",
      "Topic #6: skin product face like love use mask sunscreen tan great\n",
      "Topic #7: eye skin product cream received feel love using really serum\n",
      "\n",
      "for cluster\n",
      "Topic #0: product skin serum using received week use review love really\n",
      "Topic #1: skin love feel dry moisturizer product great face cream soft\n",
      "Topic #2: product love like skin lip smell face use sunscreen good\n",
      "Topic #3: eye cream product circle dark area love around really using\n",
      "\n",
      "Number of topics: 9\n",
      "score\n",
      "score\n",
      "0.004469496890174166\n",
      "for reduce dimension\n",
      "Topic #0: giftedbydermalogica sephoracanada dermalogicacanada mosturizer luv rlly blurry metallic dosent recommand\n",
      "Topic #1: pimple acne breakout breakouts hormonal caused product cleared brush tan\n",
      "Topic #2: skin cleanser product makeup face use love like clean gentle\n",
      "Topic #3: skin eye product using cream week use serum difference love\n",
      "Topic #4: sunscreen skin product like love face makeup spf white great\n",
      "Topic #5: tan tanner self tanning gradual orange peau que tropez produit\n",
      "Topic #6: lip smell product like balm mask love money worth good\n",
      "Topic #7: cru premier caudalie multicorrective power supreme humidifier revitalizing secret youth\n",
      "Topic #8: skin love product feel great moisturizer face cream really soft\n",
      "\n",
      "for cluster\n",
      "Topic #0: product skin love like use face smell lip really good\n",
      "Topic #1: eye cream product circle dark area love around using really\n",
      "Topic #2: skin product feel love dry great moisturizer using face really\n",
      "\n",
      "Number of topics: 10\n",
      "score\n",
      "score\n"
     ]
    }
   ],
   "source": [
    "range_topics = list(range(4, 21))\n",
    "sil_scores = []\n",
    "\n",
    "for n_topic in range_topics:\n",
    "    \n",
    "    score = pipeline(LatentDirichletAllocation, n_topic, data, 0.1, reviews_docs_joined, features)\n",
    "    if score:  # Check if score is not None\n",
    "        sil_scores.append(score)\n",
    "    else:\n",
    "        sil_scores.append(float('-inf'))  # Assign a low score for skipped iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the silhouette scores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(range_topics, sil_scores, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smunlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
