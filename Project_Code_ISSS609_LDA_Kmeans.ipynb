{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (based on prev section: k = 6, alpha = 0.9099999999999999 and beta = 0.9099999999999999) + KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import preprocess2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a Dictionary\n",
    "id2word = corpora.Dictionary(reviews_docs)\n",
    "\n",
    "# Create a Corpus\n",
    "texts = reviews_docs\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(doc) for doc in texts]\n",
    "\n",
    "# Get Document-Word matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "data = vectorizer.fit_transform(reviews_docs_joined).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_topic(corpus, model):\n",
    "    doc_topic = list()\n",
    "    for doc in corpus:\n",
    "        doc_topic.append(model.__getitem__(doc, eps=0))\n",
    "        return doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=6, \n",
    "                                       random_state=100,\n",
    "                                       alpha = 0.9099999999999999,\n",
    "                                       eta = 0.9099999999999999,\n",
    "                                       per_word_topics=True, \n",
    "                                       chunksize=100,\n",
    "                                       passes=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data = get_doc_topic(corpus, optimal_lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def gen_init_point(docTopic, docWord, a):\n",
    "\n",
    "    thershold = int((docTopic.shape[0] // docTopic.shape[1]) + a * docTopic.shape[0])\n",
    "\n",
    "    # print(thershold)\n",
    "\n",
    "    topic_mean = docTopic.mean(axis=0)\n",
    "    # print(topic_mean)\n",
    "    # print(topic_mean)\n",
    "\n",
    "    support_doc_n = []\n",
    "    support_doc_index = []\n",
    "\n",
    "    for x in range(docTopic.shape[1]):\n",
    "        topic = docTopic[:, x]\n",
    "        res_list = topic > topic_mean[x]\n",
    "        res_index = np.where(res_list == True)\n",
    "        support_doc_n.append(len(res_index[0]))\n",
    "        support_doc_index.append(res_index[0])\n",
    "\n",
    "    # print(support_doc_index)\n",
    "    # print(support_doc_n)\n",
    "    support_doc_n = np.array(support_doc_n)\n",
    "    # print(support_doc_n)\n",
    "    typical_topic = np.where(support_doc_n > thershold)[0]\n",
    "    # print(typical_topic)\n",
    "    # print(typical_topic)\n",
    "\n",
    "    k_clustering_init = []\n",
    "    for i in typical_topic:\n",
    "        # print(i)\n",
    "        # print(support_doc_index[i])\n",
    "        # print(dataset[support_doc_index[i]])\n",
    "        k_clustering_init.append(np.asarray(docWord[support_doc_index[i]].mean(axis=0)).reshape(-1))\n",
    "    return np.array(k_clustering_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words, path):\n",
    "    print('for reduce dimension')\n",
    "    out = open(path, 'w')\n",
    "    # model.components_ = lsa.inverse_transform(model.components_)\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        out.write(message+'\\n')\n",
    "    out.close()\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_cluster(model, feature, n, path):\n",
    "    print('for cluster')\n",
    "    out = open(path, 'w')\n",
    "    for topic_idx, topic in enumerate(model.cluster_centers_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature[i]\n",
    "                             for i in topic.argsort()[:-n - 1:-1]])\n",
    "        print(message)\n",
    "        out.write(message+'\\n')\n",
    "    out.close()\n",
    "    print()\n",
    "\n",
    "\n",
    "def output_result(corpus, result, path):\n",
    "    df = pd.DataFrame({'text': corpus, 'label': result})\n",
    "    df[['text', 'label']].to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from model import gen_init_point\n",
    "from model import helper\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def pipeline(model, n_topic, data, a, corpus, feature):\n",
    "    lda = model(n_components=n_topic)\n",
    "    data_lda = lda.fit_transform(data)\n",
    "    n_init_clusters = gen_init_point.gen_init_point(data_lda, data, a)\n",
    "    # print(n_init_clusters)\n",
    "    km = KMeans(n_clusters=len(n_init_clusters), init=n_init_clusters)\n",
    "    km.fit_transform(data)\n",
    "    print('score')\n",
    "    print(calinski_harabaz_score(data.toarray(), km.labels_))\n",
    "    print(silhouette_score(data.toarray(), km.labels_))\n",
    "    helper.print_top_words(lda, feature, 10, '_topic_word')\n",
    "    helper.print_cluster(km, feature, 10, '_cluster_meaning_{}')\n",
    "    helper.output_result(corpus, km.labels_, '_cluster_result_{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation,\n",
    "n_topic = 20\n",
    "pipeline.pipeline(LatentDirichletAllocation,n_topic,data,0.1,corpus,feature,'all','lda_chinese')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
