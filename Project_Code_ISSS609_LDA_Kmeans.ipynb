{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "from ast import literal_eval\n",
    "from matplotlib import colormaps\n",
    "import squarify\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import gensim\n",
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess2\n",
    "reviews_corpus = preprocess2.load_corpus('sephora_corpus')\n",
    "reviews_docs = preprocess2.corpus2docs(reviews_corpus)\n",
    "reviews_docs_joined = [\" \".join(x) for x in reviews_docs]  #joined to fit vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (based on prev section: k = 6, alpha = 0.9099999999999999 and beta = 0.9099999999999999) + KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import preprocess2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a Dictionary\n",
    "id2word = corpora.Dictionary(reviews_docs)\n",
    "\n",
    "# Create a Corpus\n",
    "texts = reviews_docs\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(doc) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Document-Word matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "data = vectorizer.fit_transform(reviews_docs_joined)\n",
    "\n",
    "# Extract features\n",
    "features = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa' 'aaa' 'aaaaah' ... 'zumba' 'zunc' 'zyleer']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_topic(corpus, model):\n",
    "    doc_topic = list()\n",
    "    for doc in corpus:\n",
    "        doc_topic.append(model.__getitem__(doc, eps=0))\n",
    "        return doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=6, \n",
    "                                       random_state=100,\n",
    "                                       alpha = 0.9099999999999999,\n",
    "                                       eta = 0.9099999999999999,\n",
    "                                       per_word_topics=True, \n",
    "                                       chunksize=100,\n",
    "                                       passes=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data = get_doc_topic(corpus, optimal_lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def gen_init_point(docTopic, docWord, a):\n",
    "\n",
    "    thershold = int((docTopic.shape[0] // docTopic.shape[1]) + a * docTopic.shape[0])\n",
    "\n",
    "    # print(thershold)\n",
    "\n",
    "    topic_mean = docTopic.mean(axis=0)\n",
    "    # print(topic_mean)\n",
    "    # print(topic_mean)\n",
    "\n",
    "    support_doc_n = []\n",
    "    support_doc_index = []\n",
    "\n",
    "    for x in range(docTopic.shape[1]):\n",
    "        topic = docTopic[:, x]\n",
    "        res_list = topic > topic_mean[x]\n",
    "        res_index = np.where(res_list == True)\n",
    "        support_doc_n.append(len(res_index[0]))\n",
    "        support_doc_index.append(res_index[0])\n",
    "\n",
    "    # print(support_doc_index)\n",
    "    # print(support_doc_n)\n",
    "    support_doc_n = np.array(support_doc_n)\n",
    "    # print(support_doc_n)\n",
    "    typical_topic = np.where(support_doc_n > thershold)[0]\n",
    "    # print(typical_topic)\n",
    "    # print(typical_topic)\n",
    "\n",
    "    k_clustering_init = []\n",
    "    for i in typical_topic:\n",
    "        # print(i)\n",
    "        # print(support_doc_index[i])\n",
    "        # print(dataset[support_doc_index[i]])\n",
    "        k_clustering_init.append(np.asarray(docWord[support_doc_index[i]].mean(axis=0)).reshape(-1))\n",
    "    return np.array(k_clustering_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words, path):\n",
    "    print('for reduce dimension')\n",
    "    out = open(path, 'w')\n",
    "    # model.components_ = lsa.inverse_transform(model.components_)\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        out.write(message+'\\n')\n",
    "    out.close()\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_cluster(model, feature, n, path):\n",
    "    print('for cluster')\n",
    "    out = open(path, 'w')\n",
    "    for topic_idx, topic in enumerate(model.cluster_centers_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature[i]\n",
    "                             for i in topic.argsort()[:-n - 1:-1]])\n",
    "        print(message)\n",
    "        out.write(message+'\\n')\n",
    "    out.close()\n",
    "    print()\n",
    "\n",
    "\n",
    "def output_result(corpus, result, path):\n",
    "    df = pd.DataFrame({'text': corpus, 'label': result})\n",
    "    df[['text', 'label']].to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def pipeline(model, n_topic, data, a, corpus, feature):\n",
    "    lda = model(n_components=n_topic)\n",
    "    data_lda = lda.fit_transform(data)\n",
    "    n_init_clusters = gen_init_point(data_lda, data, a)\n",
    "    # print(n_init_clusters)\n",
    "    km = KMeans(n_clusters=len(n_init_clusters), init=n_init_clusters)\n",
    "    km.fit_transform(data)\n",
    "    print('score')\n",
    "    print(silhouette_score(data.toarray(), km.labels_))\n",
    "    print_top_words(lda, feature, 10, '_topic_word')\n",
    "    print_cluster(km, feature, 10, '_cluster_meaning_{}')\n",
    "    output_result(corpus, km.labels_, '_cluster_result_{}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\victo\\Github\\text_analytics\\text_project\\text_analytics\\Project_Code_ISSS609_LDA_Kmeans.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/victo/Github/text_analytics/text_project/text_analytics/Project_Code_ISSS609_LDA_Kmeans.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m LatentDirichletAllocation\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/victo/Github/text_analytics/text_project/text_analytics/Project_Code_ISSS609_LDA_Kmeans.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m n_topic \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/victo/Github/text_analytics/text_project/text_analytics/Project_Code_ISSS609_LDA_Kmeans.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pipeline(LatentDirichletAllocation,n_topic,data,\u001b[39m0.1\u001b[39m,reviews_docs_joined,features)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topic = 2\n",
    "pipeline(LatentDirichletAllocation,n_topic,data,0.1,reviews_docs_joined,features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smunlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
